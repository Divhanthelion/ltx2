Training your own LoRAs is now straightforward
The official ltx-trainer package (inside the LTX-2 monorepo at packages/ltx-trainer) supports LoRA, full fine-tuning, and IC-LoRA training. GitHubGitHub Lightricks claims sub-one-hour training for motion, style, or likeness LoRAs on an H100. Hugging Face A low-VRAM configuration (ltx2_av_lora_low_vram.yaml) enables training on 32 GB GPUs using INT8 quantization and 8-bit AdamW. GitHub
Key LoRA target modules span video attention (attn1.to_k/q/v/out.0, attn2.*, ff.net.*), audio attention (audio_attn1.*, audio_attn2.*, audio_ff.*), and cross-modal layers (audio_to_video_attn.*, video_to_audio_attn.*). GitHub
Recommended training parameters from official docs and community experience converge on: rank 32–64 (minimum 32 for the 19B model to learn effectively), alpha equal to rank, learning rate 0.0002, 500–2,000 steps, AdamW optimizer, and 5% caption dropout for generalization. Hugging Face +2 Frame counts must follow the 8n+1 rule (1, 9, 17, 25…), and resolution must be divisible by 32. Hugging FaceRunComfy Cloud services (fal.ai, WaveSpeedAI) default to rank 32, 500 steps, and report final loss of 0.01–0.05 indicating successful convergence. FAL
The community trainer Lightricks/LTX-Video-Trainer (Apache 2.0) serves the earlier 2B/13B models, GitHubGitHub while a-r-r-o-w/finetrainers provides a Diffusers-based alternative GitHubGitHub that can train LTXV LoRAs on as little as 16 GB VRAM at reduced resolution. Ostris AI Toolkit is another popular option, though users report audio quality degradation when training LTX-2 LoRAs locally (GitHub Issue #684). GitHub
